{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image captioning - Master courses.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM3fyuUeK3e1DuO0/OrMXVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yscope75/CS2225.CH2001020/blob/master/Image_captioning_Master_courses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xu2ych_bSOX",
        "outputId": "e192dd4d-5c8e-44a2-dd02-dc7a1f9ee3ba"
      },
      "source": [
        "# Update torchtext version\r\n",
        "!pip3 install torchvision==0.8.0\r\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.0) (7.0.0)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.8)\n",
            "Requirement already satisfied: torchtext==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ekI-u73s7H"
      },
      "source": [
        "import torchvision.datasets as dset\r\n",
        "import torchvision.datasets.utils as dset_utils\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import os\r\n",
        "import time\r\n",
        "import math"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLRa-KjZE_3W"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torchvision\r\n",
        "from torchsummary import summary\r\n",
        "import json \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import collections\r\n",
        "from PIL import Image\r\n",
        "import torchtext\r\n",
        "from torchtext.data.utils import get_tokenizer\r\n",
        "from collections import Counter\r\n",
        "from torchtext.vocab import Vocab\r\n",
        "from torch.nn.utils.rnn import pad_sequence\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torch.optim as optim\r\n",
        "import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-WlBcZUGZuc"
      },
      "source": [
        "data_folder = os.path.join(os.path.abspath('.') + '/coco/')\n",
        "# Download and unzip annotations\n",
        "if not os.path.exists(data_folder):\n",
        "  dset_utils.download_and_extract_archive(url='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          download_root=data_folder,\n",
        "                                          extract_root=data_folder,\n",
        "                                          filename='captions.zip')\n",
        "  # Download and unzion images\n",
        "  dset_utils.download_and_extract_archive(url='http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                          download_root=data_folder,\n",
        "                                          extract_root=data_folder,\n",
        "                                          filename='train2014.zip')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmniZsp0GLdv"
      },
      "source": [
        "# Get path to file and delete zip file\r\n",
        "captions_train = os.path.join(data_folder, 'annotations/captions_train2014.json')\r\n",
        "images_train = os.path.join(data_folder, 'train2014/')\r\n",
        "captions_zip = os.path.join(data_folder, 'captions.zip')\r\n",
        "images_zip = os.path.join(data_folder, 'train2014.zip')\r\n",
        "if os.path.exists(captions_zip):\r\n",
        "  os.remove(captions_zip)\r\n",
        "if os.path.exists(images_zip):\r\n",
        "  os.remove(images_zip)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kgQnHJVQj7k"
      },
      "source": [
        "# Setup device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61GhXatKUDkX",
        "outputId": "022d5f6d-40bf-454a-cbdb-3d98068a155a"
      },
      "source": [
        "image_preprocessor = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        " \n",
        "coco_cap = dset.CocoCaptions(images_train, captions_train, transform=image_preprocessor)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.70s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6tlXZt0htr"
      },
      "source": [
        "train_slice = int(len(coco_cap)*0.6)\r\n",
        "val_slice = int(len(coco_cap)*0.2)\r\n",
        "train_data, val_data, test_data = torch.utils.data.random_split(coco_cap,\r\n",
        "                                                                [train_slice, \r\n",
        "                                                                 val_slice,\r\n",
        "                                                                 len(coco_cap)-train_slice-val_slice])\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut0yN3VV0WKu"
      },
      "source": [
        "# create dataloader for images in coco\r\n",
        "# image_loader = DataLoader(coco_cap, batch_size=10)\r\n",
        "# Loop throught images and save encoded features \r\n",
        "# encoder = Encoder()\r\n",
        "# encoder.cuda()\r\n",
        "# encoded_features = []\r\n",
        "# for id_batch, (img, caps) in enumerate(image_loader):\r\n",
        "#   encoded_batch = encoder(img.to(device))\r\n",
        "#   encoded_batch.to('cpu')\r\n",
        "#   encoded_features.append(encoded_batch)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFfw8jBRPV5s"
      },
      "source": [
        "train_captions = []\r\n",
        "\r\n",
        "for img, caps in coco_cap:\r\n",
        "  train_captions.extend(caps)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3u91Um7TtFR"
      },
      "source": [
        "# Building vocabulary for annotations\r\n",
        "tokenizer = get_tokenizer(\"basic_english\")\r\n",
        "def build_vocab(sentences, tokenizer):\r\n",
        "  counter = Counter()\r\n",
        "  for sen in sentences:\r\n",
        "    counter.update(tokenizer(sen))\r\n",
        "  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\r\n",
        "\r\n",
        "en_vocab = build_vocab(train_captions, tokenizer)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MojU9lQbJux"
      },
      "source": [
        "# Building data loader \r\n",
        "# Define params\r\n",
        "BATCH_SIZE = 24\r\n",
        "PAD_IDX = en_vocab['<pad>']\r\n",
        "BOS_IDX = en_vocab['<bos>']\r\n",
        "EOS_IDX = en_vocab['<eos>']\r\n",
        "\r\n",
        "# process batch_data\r\n",
        "def batch_process(batch_data):\r\n",
        "  img_batch, cap_batch = [], []\r\n",
        "  for img, caps in batch_data:\r\n",
        "    img_batch.extend(img.repeat(len(caps),1,1,1))\r\n",
        "    for cap in caps:\r\n",
        "      sen_ids = torch.tensor([en_vocab[token] for token in tokenizer(cap)])\r\n",
        "      cap_batch.append(torch.cat([torch.tensor([BOS_IDX]), \r\n",
        "                                 sen_ids, \r\n",
        "                                 torch.tensor([EOS_IDX])], dim=0))\r\n",
        "  cap_batch = pad_sequence(cap_batch, batch_first=True, padding_value=PAD_IDX)\r\n",
        "\r\n",
        "  return torch.stack(img_batch), cap_batch\r\n",
        "\r\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=batch_process)\r\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=batch_process)\r\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=batch_process)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUy74YyzAU7v"
      },
      "source": [
        "# Definition of main model\r\n",
        "# Begin with Encoder\r\n",
        "class Encoder(nn.Module):\r\n",
        "  \"\"\"\r\n",
        "    Encode image input using pre-trained Resnet152 model on imagenet\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    resnet152 = torchvision.models.resnet152(pretrained=True)\r\n",
        "    # remove the last two layers and keep the last CNN output \r\n",
        "    modules = list(resnet152.children())[:-2] \r\n",
        "    self.res_encoder = nn.Sequential(*modules) # last output (batch_size, 2048, 10, 10)\r\n",
        "    # Flatten feature vector to (batch_size, 2048, 100)\r\n",
        "    self.flat_embed = nn.Flatten(start_dim=2)\r\n",
        "    \r\n",
        "\r\n",
        "  def forward(self, X_in):\r\n",
        "    \"\"\"\r\n",
        "      The forward pass of encoder \r\n",
        "      args:\r\n",
        "      - X_in: input data batch of size (batch, 3, Height, weight)\r\n",
        "      return: encoded images of size (batch, embed_size, 100)\r\n",
        "    \"\"\"\r\n",
        "    e_out = self.res_encoder(X_in) \r\n",
        "    # Flatten output vector to (batch_size, embedding_size, 100)\r\n",
        "    e_out = self.flat_embed(e_out)\r\n",
        "    # Change shape of output encoded to (batch_size, 100, embedding_dim)\r\n",
        "    e_out = e_out.permute(0, 2, 1)\r\n",
        "    return e_out\r\n",
        "    \r\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ZnU43UWZ3c"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\r\n",
        "  \"\"\"\r\n",
        "    Define attention mechanism module on encoded image for genrating text\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, encoder_dim, hidden_size, attention_size):\r\n",
        "    \"\"\"\r\n",
        "      args: \r\n",
        "      - encoder_dim: size of encoded image (batch_size, 100, embedding_size(2048))\r\n",
        "      - hidden_size: size of hidden unit in decoder RNN \r\n",
        "      - attention_size: \r\n",
        "    \"\"\"\r\n",
        "    super(BahdanauAttention, self).__init__()\r\n",
        "    self.W1 = nn.Linear(encoder_dim, attention_size)   # size\r\n",
        "    self.W2 = nn.Linear(hidden_size, attention_size)\r\n",
        "    self.V = nn.Linear(attention_size, 1)\r\n",
        "    \r\n",
        "  def forward(self, encoded_feature, hidden):\r\n",
        "    \"\"\"\r\n",
        "      args:\r\n",
        "      - encoded_feature: \r\n",
        "    \"\"\"\r\n",
        "    # expand time dimension for hidden layer in decoder (batch_size, 1, hidden_size)\r\n",
        "    # Compute attenntion for hidden (batch_size, 100, attention_size)\r\n",
        "    attention_on_hidden = torch.tanh(self.W1(encoded_feature) + self.W2(hidden))\r\n",
        "    # attention score on attention (batch, 100, 1)\r\n",
        "    score = self.V(attention_on_hidden)\r\n",
        "    # compute attention weights \r\n",
        "    attention_weights = torch.softmax(score, dim=1)\r\n",
        "    context_vector = attention_weights*encoded_feature\r\n",
        "    context_vector = torch.sum(context_vector, dim=1)\r\n",
        "    \r\n",
        "    return context_vector, attention_weights\r\n",
        "    \r\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAoP-fPiaDmI"
      },
      "source": [
        "torch.manual_seed(3)\n",
        "class DecoderWithAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim,\n",
        "               hidden_size,\n",
        "               vocab_size,\n",
        "               encoded_dim,\n",
        "               attention: nn.Module,\n",
        "               pretrained_embed=None):\n",
        "    super(DecoderWithAttention, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.encoded_dim = encoded_dim\n",
        "    self.embedding = self.init_embedding(pretrained_embed)\n",
        "    self.gru_in = self.embedding_dim + self.encoded_dim\n",
        "    self.gru = nn.GRU(input_size=self.gru_in,\n",
        "                      hidden_size=self.hidden_size,\n",
        "                      batch_first=True)\n",
        "    self.fc1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "    self.fc2 = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "    \n",
        "    self.attention = attention\n",
        "    \n",
        "  def init_embedding(self, weight):\n",
        "    \"\"\"\n",
        "      if pretrained embedding exists then load from pretrained\n",
        "      else load from new one\n",
        "    \"\"\"\n",
        "    embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "    if weight is not None:\n",
        "      embedding = nn.Embedding.from_pretrained(weight)\n",
        "    \n",
        "    return embedding\n",
        " \n",
        "  def forward(self, encoded_features, x, hidden):\n",
        "    \n",
        "    # get necessary size\n",
        "    batch_size = encoded_features.size(0)\n",
        "    # compute context vector and attention weights \n",
        "    # context_vector: (batch_size, visual_embedding_size(2048))\n",
        "    context_vector, atten_weights = self.attention(encoded_features, hidden.unsqueeze(1))\n",
        "    # embed token x to vector\n",
        "    # x: (batch, embedding_size)\n",
        "    x = self.embedding(x).unsqueeze(1)\n",
        "    # Concatinate context vector to input \n",
        "    x = torch.cat((x, context_vector.unsqueeze(1)), dim=-1)\n",
        "    # output size: (batch_size, sequence_len, hidden_size)\n",
        "    output, hn = self.gru(x, hidden.unsqueeze(0))\n",
        "    output = output.squeeze(1)\n",
        "    x = self.fc1(output)\n",
        "    # Change size to (batch_size*sequence_len, hidden_size)\n",
        "    x = x.view(-1, x.size()[-1])\n",
        "    x = self.fc2(x)\n",
        " \n",
        "    return x, hn.squeeze(), atten_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB18mQg7pXA9"
      },
      "source": [
        "class Captioning(nn.Module):\r\n",
        "  def __init__(self,\r\n",
        "               encoder: nn.Module,\r\n",
        "               decoder: nn.Module,\r\n",
        "               device: torch.device):\r\n",
        "    super().__init__()\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "    self.device = device\r\n",
        "    \r\n",
        "  def forward(self, \r\n",
        "              img_src: torch.Tensor,\r\n",
        "              trg: torch.Tensor,\r\n",
        "              teacher_forc_rate: float=0.5):\r\n",
        "    batch_size = img_src.shape[0]\r\n",
        "    max_len = trg.shape[1]\r\n",
        "\r\n",
        "    trg_vocab_size = self.decoder.vocab_size\r\n",
        "\r\n",
        "    outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(self.device)\r\n",
        "    encoder_out = self.encoder(img_src)\r\n",
        "    hidden = self.init_hidden(batch_size).to(device)\r\n",
        "    output = trg[:, 0]\r\n",
        "    for t in range(1, max_len):\r\n",
        "      output, hidden, _ = self.decoder(encoder_out, output, hidden)\r\n",
        "      outputs[:,t,:] = output\r\n",
        "      teacher_force = random.random() < teacher_forc_rate \r\n",
        "      top = output.max(1)[1]\r\n",
        "      output = (trg[:,t] if teacher_force else top)\r\n",
        "      \r\n",
        "    return outputs\r\n",
        "\r\n",
        "  def init_hidden(self, batch_size):\r\n",
        "    w = torch.empty(batch_size, self.decoder.hidden_size)\r\n",
        "    return nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOWhn9qwX2cf"
      },
      "source": [
        "# Define training parameters \r\n",
        "OUTPUT_DIM = len(en_vocab)\r\n",
        "EMB_DIM = 256\r\n",
        "HIDDEN_DIM = 512\r\n",
        "ATTEN_DIM = 512\r\n",
        "ENCODE_DIM = 2048"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmOYcHUBdmZl"
      },
      "source": [
        "encoder = Encoder()\r\n",
        "attention = BahdanauAttention(ENCODE_DIM, HIDDEN_DIM, ATTEN_DIM)\r\n",
        "decoder = DecoderWithAttention(EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, ENCODE_DIM, attention)\r\n",
        "model = Captioning(encoder, decoder, device).to(device)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSg6wxHwlqx"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\r\n",
        "for param in encoder.parameters():\r\n",
        "  param.requires_grad = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsU8DApaKY0-"
      },
      "source": [
        "PAD_IDX = en_vocab.stoi['<pad>']\r\n",
        "loss_func = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyzcR9nPK5IO"
      },
      "source": [
        "def train(model: nn.Module,\r\n",
        "          iterator: torch.utils.data.DataLoader,\r\n",
        "          optimizer: optim.Optimizer,\r\n",
        "          loss_func: nn.Module):\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  epoch_loss = 0\r\n",
        "\r\n",
        "  for _, (img, trg) in enumerate(iterator):\r\n",
        "      img = img.to(device)\r\n",
        "      trg = trg.to(device)\r\n",
        "\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "      output = model(img, trg)\r\n",
        "\r\n",
        "      output = output[:,1:,:].contiguous().view(-1, output.shape[-1])\r\n",
        "      trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "      loss = loss_func(output, trg)\r\n",
        "\r\n",
        "      loss.backward()\r\n",
        "\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "      epoch_loss += loss.item()\r\n",
        "\r\n",
        "  return epoch_loss / len(iterator)\r\n",
        "\r\n",
        "def evaluate(model: nn.Module,\r\n",
        "             iterator: torch.utils.data.DataLoader,\r\n",
        "             loss_func: nn.Module):\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  epoch_loss = 0\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "\r\n",
        "      for _, (src, trg) in enumerate(iterator):\r\n",
        "          src, trg = src.to(device), trg.to(device)\r\n",
        "\r\n",
        "          output = model(src, trg, 0) \r\n",
        "\r\n",
        "          output = output[1:].contiguous().view(-1, output.shape[-1])\r\n",
        "          trg = trg[1:].contiguous().view(-1)\r\n",
        "\r\n",
        "          loss = loss_func(output, trg)\r\n",
        "\r\n",
        "          epoch_loss += loss.item()\r\n",
        "\r\n",
        "  return epoch_loss / len(iterator)\r\n",
        "\r\n",
        "def epoch_time(start_time: int,\r\n",
        "               end_time: int):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPVbn4ce-bAo"
      },
      "source": [
        "# define checkpoint info\r\n",
        "CKP_PATH = 'model.pt'\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2EwMNAaMQEW",
        "outputId": "f0bd55ae-91cb-4988-89a8-7d723e94b5f0"
      },
      "source": [
        "N_EPOCHS = 10\n",
        " \n",
        "best_valid_loss = float('inf')\n",
        " \n",
        "for epoch in range(N_EPOCHS):\n",
        " \n",
        "    start_time = time.time()\n",
        " \n",
        "    train_loss = train(model, train_iter, optimizer, loss_func)\n",
        "    valid_loss = evaluate(model, valid_iter, loss_func)\n",
        "    if epoch % 4 == 0:\n",
        "      torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': train_loss,\n",
        "        }, CKP_PATH)\n",
        "    end_time = time.time()\n",
        " \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        " \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        " \n",
        "test_loss = evaluate(model, test_iter, loss_func)\n",
        " \n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 49m 45s\n",
            "\tTrain Loss: 4.178 | Train PPL:  65.212\n",
            "\t Val. Loss: 4.930 |  Val. PPL: 138.439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf1_axYxKAnL"
      },
      "source": [
        "def predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D87DgAWZXq3"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cLT0XfCdo2A"
      },
      "source": [
        ""
      ],
      "execution_count": 172,
      "outputs": []
    }
  ]
}