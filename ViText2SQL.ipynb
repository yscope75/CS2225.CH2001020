{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViText2SQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTWfVtQZ0OXylA7pf5Ll0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yscope75/CS2225.CH2001020/blob/master/ViText2SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53jlspp4cONT"
      },
      "source": [
        "import abc\n",
        "import attr\n",
        "import json\n",
        "import networkx as nx\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csgI36b_ciVS"
      },
      "source": [
        "=============================ViTest2Sql Loader================================"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59a0XHNwceog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "9fc62971-9673-4422-9cb4-a10cfaf4dddf"
      },
      "source": [
        "# Adapted from RatSQL\n",
        "@attr.s\n",
        "class ViText2SQLItem:\n",
        "  text = attr.ib()\n",
        "  code = attr.ib()\n",
        "  schema = attr.ib()\n",
        "  orig = attr.ib()\n",
        "  orig_schema = attr.ib()\n",
        "\n",
        "@attr.s\n",
        "class Column:\n",
        "  id = attr.ib()\n",
        "  table = attr.ib()\n",
        "  name = attr.ib()\n",
        "  unsplit_name = attr.ib()\n",
        "  orig_name = attr.ib()\n",
        "  type = attr.ib()\n",
        "  foreign_key_for = attr.ib(default=None)\n",
        "  \n",
        "@attr.s\n",
        "class Table:\n",
        "  id = attr.ib()\n",
        "  name = attr.ib()\n",
        "  unsplit_name = attr.ib()\n",
        "  orig_name = attr.ib()\n",
        "  columns = attr.ib(factory=list)\n",
        "  primary_keys = attr.ib(factory=list)\n",
        "  \n",
        "@attr.s\n",
        "class Schema:\n",
        "  db_id = attr.ib()\n",
        "  tables = attr.ib()\n",
        "  columns = attr.ib()\n",
        "  foreign_key_graph = attr.ib()\n",
        "  orig = attr.ib()\n",
        "  connection = attr.ib(default=None)\n",
        "\n",
        "def load_tables(paths)\n",
        "  schemas = {}\n",
        "  eval_foreign_key_maps = {}\n",
        "  \n",
        "  for path in paths:\n",
        "    schema_dicts = json.load(open(path))\n",
        "    for schema_dict in schema_dicts:\n",
        "      tables = tuple(\n",
        "          Table(\n",
        "              id=i,\n",
        "              name=name.split() # syllabus level problem here\n",
        "              unsplit_name=name,\n",
        "              orig_name=orig_name) \n",
        "          for i, (name, orig_name) in enumerate(zip(\n",
        "              schema_dict['table_names'], schema_dict['table_names_original']\n",
        "          )))\n",
        "      \n",
        "      columns = tupple( \n",
        "          Column(\n",
        "              id=i,\n",
        "              table=tables[table_id] if table_id >= 0 else None,\n",
        "              name=col_name.split(), # for word level\n",
        "              unsplit_name=orig_col_name,\n",
        "              type=col_type\n",
        "          )for i, ((table_id, col_name), (_, orig_col_name), col_type) from zip(schema_dict['column_names'], \n",
        "                            schema_dict['colum_names_original'], \n",
        "                            schema_dict['column_types']))\n",
        "      \n",
        "    # Link tables and columns\n",
        "    for column in columns:\n",
        "      if column.table:\n",
        "        column.table.columns.append(column)\n",
        "    \n",
        "    # register primary keys\n",
        "    for column_id in schema_dict['primary_key']:\n",
        "      column = columns[column_id]\n",
        "      column.table.primary_keys.append(column)\n",
        "\n",
        "    foreign_key_graph = nx\n",
        "    for source_column_id, dest_column_id in schema_dict['foreign_keys']:\n",
        "      source_column = columns[source_column_id]\n",
        "      dest_column = columns[dest_column_id]\n",
        "      source_column.foreign_key_for = dest_column\n",
        "      foreign_key_graph.add_edge(\n",
        "          source_column.table.id,\n",
        "          dest_column.table.id,\n",
        "          columns=(source_column_id, dest_column_id)\n",
        "      )\n",
        "      foreign_key_graph.add_edge(\n",
        "          dest_column.table.id,\n",
        "          source_column.table.id, \n",
        "          columns=(dest_column_id, source_column_id)\n",
        "      )\n",
        "      assert db_id not in schemas \n",
        "      schemas[db_id] = Schema(db_id, tables, columns, foreign_key_graph, schema_dict)\n",
        "      # To do: for evaluation\n",
        "    \n",
        "  return schemas\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "  def __init__(self, paths, tables_paths, db_path=None, limit=None):\n",
        "    self.paths = paths\n",
        "    self.db_path = db_path\n",
        "    self.examples = []\n",
        "\n",
        "    self.schemas = load_tables(tables_paths)\n",
        "    \n",
        "    for path in paths:\n",
        "      raw_data = json.load(open(path))\n",
        "      for entry in raw_data:\n",
        "        item = ViText2SQLItem(\n",
        "            text=entry['question_tokens'],\n",
        "            code=entry['sql'],\n",
        "            schema=self.schemas[entry[db_id]],\n",
        "            orig=entry,\n",
        "            orig_schema=self.schemas[entry[db_id]].orig)\n",
        "        self.examples.append(item)\n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.examples[idx]\n",
        "\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-61f96a296582>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    def load_tables(paths)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g6rAKMRT2wt"
      },
      "source": [
        "# abstract class for data preprocessor adapted from RATSQL\n",
        "class AbstractPreproc(metaclass=abc.ABCMeta):\n",
        "  '''Used for preprocessing data according to the model's liking.\n",
        "\n",
        "  Some tasks normally performed here:\n",
        "  - Constructing a vocabulary from the training data\n",
        "  - Transforming the items in some way, such as\n",
        "      - Parsing the AST\n",
        "      - \n",
        "  - Loading and providing the pre-processed data to the model\n",
        "\n",
        "  TODO:\n",
        "  - Allow transforming items in a streaming fashion without loading all of them into memory first\n",
        "  '''\n",
        "  \n",
        "  @abc.abstractmethod\n",
        "  def validate_item(self, item, section):\n",
        "      '''Checks whether item can be successfully preprocessed.\n",
        "      \n",
        "      Returns a boolean and an arbitrary object.'''\n",
        "      pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def add_item(self, item, section, validation_info):\n",
        "      '''Add an item to be preprocessed.'''\n",
        "      pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def clear_items(self):\n",
        "      '''Clear the preprocessed items'''\n",
        "      pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def save(self):\n",
        "      '''Marks that all of the items have been preprocessed. Save state to disk.\n",
        "\n",
        "      Used in preprocess.py, after reading all of the data.'''\n",
        "      pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def load(self):\n",
        "      '''Load state from disk.'''\n",
        "      pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def dataset(self, section):\n",
        "      '''Returns a torch.data.utils.Dataset instance.'''\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLOf2lH0Qrw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}